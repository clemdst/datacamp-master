{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the box of gradient boosting decision trees (GBDT)\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```\n",
    "\n",
    "The aim of this notebook is 2-folds:\n",
    "\n",
    "- dissecting a pure Python (unoptimized) implementation of GBRT\n",
    "- use a profiler (here snakeviz) to identify the computational bottleneck.\n",
    "\n",
    "We will explore the code in the file `tinygbt.py`.\n",
    "\n",
    "TinyGBT (Tiny Gradient Boosted Trees) is a 200 line gradient boosted trees implementation written in pure Python.\n",
    "\n",
    "First let's load some regression data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Adaboost** : changes the weight on the misclassified samples at each step, closed form for the step for each point\n",
    "\n",
    "- **Gradient boosting** : Predictor built on several iterations. First order Taylor approximation (*first order perturbation*) of the error term. We have closed form for the gradient in conditional expectation, but unsolvable, switch to a regression problem where we minimize MSE : regression ON THE GRADIENT even for classification ! We approximate gradient and compute step $\\alpha$ with line search. The estimator at step $m$ will then be $$F_m = F_{m-1} + \\alpha g$$\n",
    "\n",
    "In the case of gradient boosting, parameters are $(A_j, b_j)$ the feature space and weights. Gradient depends on $A$ and $b$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('datasets/regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('datasets/regression.test', header=None, sep='\\t')\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "y = df[0].values\n",
    "X = df.drop(0, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in train and test. Here the test data will be\n",
    "used as **validation set** to do **\"early stopping\"** i.e. to stop adding trees\n",
    "to the model if the validation loss increases for multiple successive boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run our TinyGBT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.3316037269, Valid's L2: 0.3300072545, Elapsed: 5.25 secs\n",
      "Iter   1, Train's L2: 0.1985378411, Valid's L2: 0.2388153401, Elapsed: 5.00 secs\n",
      "Iter   2, Train's L2: 0.1889184203, Valid's L2: 0.2318430329, Elapsed: 4.98 secs\n",
      "Iter   3, Train's L2: 0.1862566024, Valid's L2: 0.2304068606, Elapsed: 5.05 secs\n",
      "Iter   4, Train's L2: 0.1857969147, Valid's L2: 0.2302255208, Elapsed: 5.05 secs\n",
      "Iter   5, Train's L2: 0.1856362275, Valid's L2: 0.2301637301, Elapsed: 4.93 secs\n",
      "Iter   6, Train's L2: 0.1855901546, Valid's L2: 0.2301413405, Elapsed: 4.94 secs\n",
      "Iter   7, Train's L2: 0.1855760029, Valid's L2: 0.2301347475, Elapsed: 5.04 secs\n",
      "Iter   8, Train's L2: 0.1855717778, Valid's L2: 0.2301326823, Elapsed: 5.01 secs\n",
      "Iter   9, Train's L2: 0.1855704912, Valid's L2: 0.2301321590, Elapsed: 5.05 secs\n",
      "Training finished. Elapsed: 50.30 secs\n",
      "Start predicting...\n",
      "The MSE of prediction is: 0.23013215902637066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tinygbt import GBT\n",
    "\n",
    "print('Start training...')\n",
    "gbt = GBT(n_estimators=10, gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "y_pred = gbt.predict(X_test)\n",
    "print('The MSE of prediction is:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure this code gives comparable results with [lightgbm](https://lightgbm.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's l2: 0.217964\n",
      "Saving model...\n",
      "Starting predicting...\n",
      "The MSE of prediction is: 0.21796435968378255\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 1.,\n",
    "    'bagging_fraction': 1.,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_eval,\n",
<<<<<<< HEAD
    "                callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
=======
    "                callbacks=[lgb.early_stopping(stopping_rounds=3)]\n",
    "               )\n",
>>>>>>> 18597da3bf0d9df9bed5483ea75098bca3ac1ae6
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also with scikit-learn [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of prediction is: 0.19818811241253062\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.experimental import enable_hist_gradient_boosting  # uncomment this for sklearn < 1.0\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "est = HistGradientBoostingRegressor(\n",
    "    max_iter=10,\n",
    "    validation_fraction=0.2,\n",
    "    random_state=42,\n",
    "    l2_regularization=0.,\n",
    "    min_samples_leaf=20,\n",
    "    learning_rate=0.3,\n",
    "    n_iter_no_change=5).fit(X_train, y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a round of profiling using snakeviz\n",
    "\n",
    "While lightgbm is implemented in C++ and HistGradientBoostingRegressor with Cython, our TinyGBT is implemented in pure Python. The consequence is that our code is much slower. To identify the computational bottleneck let's profile our code. For this we will use snakeviz.\n",
    "\n",
    "There are many other profilers such as:\n",
    "- line_profiler: https://github.com/pyutils/line_profiler (easy line by line profiling for Python code)\n",
    "- viztracer: https://viztracer.readthedocs.io (allows to profile low level code including compiled Cython code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.3313507510, Valid's L2: 0.3304099775, Elapsed: 8.35 secs\n",
      "Iter   1, Train's L2: 0.1844998134, Valid's L2: 0.2274262735, Elapsed: 6.97 secs\n",
      "Training finished. Elapsed: 15.33 secs\n",
      " \n",
      "*** Profile stats marshalled to file 'C:\\\\Users\\\\33768\\\\AppData\\\\Local\\\\Temp\\\\tmp2y0u0442'.\n",
      "Embedding SnakeViz in this document...\n",
      "<function display at 0x0000020CBEA979C0>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-f3645a4a-db34-11f0-9fd7-c53f5701464f' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-f3645a4a-db34-11f0-9fd7-c53f5701464f\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/C%3A%5CUsers%5C33768%5CAppData%5CLocal%5CTemp%5Ctmp2y0u0442\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext snakeviz\n",
    "\n",
    "gbt = GBT(n_estimators=2,  # do only two rounds of boosting\n",
    "          gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "\n",
    "%snakeviz gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Where is most of the time spent?</li>\n",
    "         <li>What is the complexity of the <code>TreeNode.build</code> method as a function of the number of samples and the number of features?</li>\n",
    "         <li>What could we do about this?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complexity $\\mathcal{O}(\\mathcal{D} \\cdot \\underbrace{n_{samples}}_\\text{due to sorting} \\cdot log(n))$\n",
    "\n",
    "Sorting features is what takes the most time. We sort once at the beginning using bins, group values around similar values ; a bit of loss in performance but huge gain in timing. Now complexity is $\\mathcal{O}(\\mathcal{D} \\cdot n_{bins} \\cdot log(n))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.6"
=======
   "version": "3.14.2"
>>>>>>> 18597da3bf0d9df9bed5483ea75098bca3ac1ae6
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
